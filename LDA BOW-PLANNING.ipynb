{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "743d9dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep a list of specific words/phrases you want to remove so as to focus on topic modelling\n",
    "# World Heritage Site (WHS), Special Area of Conservation (SAC), Area of Great Landscape Value\n",
    "\n",
    "\n",
    "text_to_remove = [\n",
    "        'The Local Planning Authority has acted positively and proactively in determining this application by identifying matters of concern with this proposal.',\n",
    "        ' On this occasion, the issues are so fundamental that it is not possible to negotiate a satisfactory way forward due to the harm that has been clearly identified within the reason(s) for refusal.',\n",
    "        'January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December',\n",
    "        \"National Planning Policy Framework\",\"NPPF\",\n",
    "        \"building\", \"built\",\n",
    "        \"special justification\",\n",
    "        \"demonstrate\",\n",
    "        \"house\", \"permission\", \"condition\",\n",
    "        \"section\",\n",
    "        \"work\",\n",
    "        \"intentions\",\n",
    "        \"adopted\",\n",
    "        \"adoption\",\n",
    "        \"represent\",\n",
    "        \"representing\",\n",
    "        \"settlement\",\n",
    "        \"planning\",\n",
    "        \"Cornwall\",\n",
    "        \"local\",\n",
    "        \"plan\",\n",
    "        \"development\",\n",
    "        \"policies\",\n",
    "        \"contrary\",\n",
    "        \"proposal\",\n",
    "        \"application\",\n",
    "        \"policy\",\n",
    "        \"policies\",\n",
    "        \"paragraph\",\"paragraphs\",\n",
    "        \"permitted\",\n",
    "        \"development\",\n",
    "        \"area\",\n",
    "        \"dwelling\",\n",
    "        \"proposed\",\n",
    "        \"national\",\n",
    "        \"house\",\n",
    "        \"housing\",\n",
    "        \"justify\",\n",
    "        \"town\",\n",
    "        \"constitute\",\n",
    "        \"sited\",\n",
    "        \"siting\",\n",
    "        \"guidance\",\n",
    "        \"benefit\",\n",
    "        \"justification\",\n",
    "        \"point\",\n",
    "        \"raise\",\n",
    "        \"factor\",\n",
    "        \"balance\",\n",
    "        \"house\",\n",
    "        \"identify\",\n",
    "        \"fall\",\n",
    "        \"land \",\n",
    "        \"provide\",\n",
    "        \"circumstances\",\n",
    "        \"considerable\",\n",
    "        \"tree\",\n",
    "        \"resulting\",\n",
    "        \"site\", \n",
    "        \"result\", \n",
    "        \"reason\",\n",
    "        \"Caradon\", \"consider\", \"conflict\", 'aim', 'affect', 'impact'\n",
    "        #\"form\" can't have form because of \"information\"\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2294b343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the list so it is greedy when it later removes entries and doesn't take partial words\n",
    "text_to_remove = sorted(text_to_remove, key=len, reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08cacd5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n(0) Load df, trim columns. Remove unwanted record categories (trees, lawfulness etc)\\n(1) Extract useful data like NPD references and permissions references and add as columns\\n(1) Preprocess text (remove particular words, remove stopwords, lemmatize)\\n(2) Create dictionary with an int ID for each remaining word\\n(3) Filter the dictionary down if desired, removing very rare or very common words\\n(4) Create BOW for each doc - a list of tuples for each record:\\n    [[(int ref from the dictionary, word frequency)],[(int ref from the dictionary, word frequency)]]\\n(5) Train the lda model, providing the dictionary, no of topics, no of passes etc.\\n(6) Adjust preprocessing steps or variables in (5) to tweak topic model output to meaningful topics\\n(7) Topic model returns a list of topics, with each topic containing tuples of the word id and the probabilities\\nthat the word will feature in a text of that topic\\n(8) Create a topic probabilities dataframe, with each topic in columns and values containing probabilities\\nHOW DOES THIS GET TO A SINGLE PROBABILITY PER RECORD, RATHER THAN PROBABILITIES BY WORD???\\n(9) Add the topic probabilites to original dataframe\\n\\n\\nTO DO:\\n\\n10) Add further columns turning probabilities into boolean values at a given cut off point\\n11) Add logging feature to track outputs with given variables to improve tweaking performance\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "(0) Load df, trim columns. Remove unwanted record categories (trees, lawfulness etc)\n",
    "(1) Extract useful data like NPD references and permissions references and add as columns\n",
    "(1) Preprocess text (remove particular words, remove stopwords, lemmatize)\n",
    "(2) Create dictionary with an int ID for each remaining word\n",
    "(3) Filter the dictionary down if desired, removing very rare or very common words\n",
    "(4) Create BOW for each doc - a list of tuples for each record:\n",
    "    [[(int ref from the dictionary, word frequency)],[(int ref from the dictionary, word frequency)]]\n",
    "(5) Train the lda model, providing the dictionary, no of topics, no of passes etc.\n",
    "(6) Adjust preprocessing steps or variables in (5) to tweak topic model output to meaningful topics\n",
    "(7) Topic model returns a list of topics, with each topic containing tuples of the word id and the probabilities\n",
    "that the word will feature in a text of that topic\n",
    "(8) Create a topic probabilities dataframe, with each topic in columns and values containing probabilities\n",
    "HOW DOES THIS GET TO A SINGLE PROBABILITY PER RECORD, RATHER THAN PROBABILITIES BY WORD???\n",
    "(9) Add the topic probabilites to original dataframe\n",
    "\n",
    "\n",
    "TO DO:\n",
    "\n",
    "10) Add further columns turning probabilities into boolean values at a given cut off point\n",
    "11) Add logging feature to track outputs with given variables to improve tweaking performance\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31b22a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(400)\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69421e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('wordnet') # might not be needed once run once\n",
    "#nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "162d1617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataframe from an excel file. For now keep an unedited copy and one to manipulate\n",
    "og_df = pd.read_csv(\"/Users/GlassShark1/Python/Refusals Data/All Refusals 2019-2022 w devtypes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71685bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = og_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6cf3145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jupyter_display_max(max_rows):\n",
    "    # set display so you can see all columns, all rows and all cell contents (up to 1k characters)\n",
    "    pd.options.display.max_columns = None\n",
    "    pd.set_option('display.max_rows', max_rows)\n",
    "    pd.options.display.max_colwidth = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a74c6e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_col_by_labels(df,column_names):\n",
    "    for col in column_names:\n",
    "        if col in df.columns:\n",
    "            df = df.drop(col, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "daaaa8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_df_down(df):\n",
    "    \n",
    "    #Define attribute columns with data on the permission itself\n",
    "    info_cols = ['Unnamed: 0', 'Address', 'docfragment', 'file_ext', 'filename', 'Decision', 'Link','FromSearch', 'ToSearch', 'AppTypeFrag']\n",
    "    \n",
    "    # Define columns to keep for the NLP work\n",
    "    keep_cols = ['Ref', 'Description', 'Dev_Type','DecDate', 'RefusalReasons']\n",
    "    \n",
    "    # find any other columns not in the above categories to remove - these will be those already manually categorised\n",
    "    x_train_cols = [col for col in df.columns if col not in info_cols and col not in keep_cols]\n",
    "    \n",
    "    # for unsupervised model, remove unneeded cols + manually categorised data\n",
    "    remove_cols = info_cols + x_train_cols\n",
    "    \n",
    "    # remove unwanted columns\n",
    "    df = drop_col_by_labels(df,remove_cols)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52fdd81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_numbers(text):\n",
    "    \n",
    "    # create a list of characters if the character is a letter or a space\n",
    "    strip_numbers = [char for char in text if char.isalpha() or char == \" \"]\n",
    "    \n",
    "    # join the characters again with 'nothing' - as spaces are included above\n",
    "    strip_numbers = \"\".join(strip_numbers)\n",
    "    \n",
    "    # return the string\n",
    "    return strip_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b317c690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_specific_text(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "    # removes common/unwanted/not valuable text and phrases from string\n",
    "    \n",
    "    # for each thing you want to remove from the text\n",
    "    for phrase in master_list:\n",
    "        # if the lower case version is in the lower case version of the text, replace it with nothing (delete)\n",
    "        if phrase.lower() in text.lower():\n",
    "            #print(phrase, \" in text\")\n",
    "            text = text.replace(phrase.lower(), \"\")\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4a54aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple func to return length. Used to apply to a list of values\n",
    "def leng_func(x):\n",
    "    return len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd4a1b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Perm_regex(text):\n",
    "    \n",
    "    # Identify permission references\n",
    "    matchtype = r'(?i)(?:PA)?\\d{2}[/|_]\\d{5}' # e.g. PA12/12345\n",
    "    matches = re.findall(matchtype, text)\n",
    "    \n",
    "    # sort the list by highest number of characters first. This means CLPSP will be replaced before CLP, \n",
    "    # so you aren't left with instances of SP on their own\n",
    "    matches.sort(reverse = True,key=leng_func)\n",
    "    \n",
    "    # Don't return anything if it's an empty list / no matches\n",
    "    if matches ==[]:\n",
    "        matches = \"\"\n",
    "    \n",
    "    # remove dups by set and turn into a single comma seperated string\n",
    "    matches = set(matches)\n",
    "    matches = \",\".join(matches)\n",
    "\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23da78de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CLP_regex(text):\n",
    "    matchtype = r'(Cornwall Local Plan Strategic Policies|Cornwall Local Plan|CLP) (?:\\d{4}\\s*-\\s*\\d{4}|\\d{4}?)?'\n",
    "    #matchtype = r'(Cornwall Local Plan Strategic Policies|Cornwall Local Plan|CLP)\\s*((\\(\\d{4}(?:\\s*-\\s*\\d{4})?\\))|\\d{4}\\s*-\\s*\\d{4}|\\d{4})?'\n",
    "    matches = re.findall(matchtype, text)\n",
    "    \n",
    "    # sort the list by highest number of characters first. This means CLPSP will be replaced before CLP, \n",
    "    # so you aren't left with instances of SP on their own\n",
    "    matches.sort(reverse = True,key=leng_func)\n",
    "    \n",
    "    # Don't return anything if it's an empty list / no matches\n",
    "    if matches ==[]:\n",
    "        matches = \"\"\n",
    "        \n",
    "    # remove dups by set and turn into a single comma seperated string\n",
    "    matches = set(matches)\n",
    "    matches = \",\".join(matches)\n",
    "        \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9d07d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AONB_regex(text):\n",
    "    matchtype = r'(?i)\\barea\\s*of\\s*outstanding\\s*natural\\s*beauty\\b|\\bAONB\\b|\\(AONB\\)'\n",
    "    matches = re.findall(matchtype, text)\n",
    "    \n",
    "    # sort the list by highest number of characters first. This means CLPSP will be replaced before CLP, \n",
    "    # so you aren't left with instances of SP on their own\n",
    "    matches.sort(reverse = True,key=leng_func)\n",
    "    \n",
    "    # Don't return anything if it's an empty list / no matches\n",
    "    if matches ==[]:\n",
    "        matches = \"\"\n",
    "        \n",
    "    # remove dups by set and turn into a single comma seperated string\n",
    "    matches = set(matches)\n",
    "    matches = \",\".join(matches)\n",
    "        \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3ab20e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NPD_regex(text):\n",
    "    matchtype = r'\\b(?:[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)\\s+(?:N(?:eighbourhood)?\\s*(?:D(?:evelopment)?\\s*)?P(?:lan)?|NDP)\\b(?:\\s+\\([^\\)]+\\))?(?:\\s+\\d{4}(?:\\s*(?:to|-)\\s*\\d{4})?)?'    \n",
    "    matches = re.findall(matchtype, text)\n",
    "    \n",
    "    # sort the list by highest number of characters first. This means CLPSP will be replaced before CLP, \n",
    "    # so you aren't left with instances of SP on their own\n",
    "    matches.sort(reverse = True,key=leng_func)\n",
    "    \n",
    "    # Don't return anything if it's an empty list / no matches\n",
    "    if matches ==[]:\n",
    "        matches = \"\"\n",
    "        \n",
    "    # remove dups by set and turn into a single comma seperated string\n",
    "    matches = set(matches)\n",
    "    matches = \",\".join(matches)\n",
    "        \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b824c113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saints_rule(text):\n",
    "    # removes full stop in St. before extracting parish names so it is not read as a full stop\n",
    "    if 'St. ' in text:\n",
    "        text.replace('St. ', 'St ')\n",
    "    #specific case where the word 'form' skews results, but we don't want to lose 'information'\n",
    "    if ' form ' in text:\n",
    "        text.replace( ' form ', ' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b87e3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(text):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    return stemmer.stem(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03d5db9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    return WordNetLemmatizer().lemmatize(text, pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d4d096e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords_and_preprocess(text):\n",
    "    # creates a list of all words passed in if they are not stopwords or v.sml and returns lemmatized version\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            # specifiy preprocessing steps here - stemming / lemmatization, both\n",
    "            # result.append(lemmatize_stemming(token))\n",
    "            # result.append(stemming(token))\n",
    "            result.append(lemmatize(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c072a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_all_values_in_col(col):\n",
    "    # return a set of all the different values (all values represented only once)\n",
    "    all_list = set(df[col].tolist())\n",
    "    # turn the set into a list\n",
    "    all_list = [item for item in all_list]\n",
    "    \n",
    "    # from the list that can contain sublists, flatten into single list\n",
    "    new_list = []\n",
    "    for item in all_list:\n",
    "        if item == '':\n",
    "            continue\n",
    "        sublist = item.split(',')\n",
    "        for item in sublist:\n",
    "            new_list.append(item)\n",
    "    \n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e8d1f0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that takes a set of values to be replaced with a single value\n",
    "def replace_text(text, string_set, returnstr):\n",
    "    for string in string_set:\n",
    "        text = text.replace(string, returnstr)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9aa7d370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set JN to display full extent of data\n",
    "jupyter_display_max(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24a7998c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chop the df down to only the bits you will use\n",
    "df = cut_df_down(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "638b5398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a lemmatized version of the list\n",
    "text_to_remove2 = [lemmatize(word) for word in text_to_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "65bae7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of confusing 'St.' strings\n",
    "df[\"RefusalReasons\"] = df[\"RefusalReasons\"].apply(saints_rule)\n",
    "# create a column containing a string of references to NPDs\n",
    "df['NDPs Referenced'] = df[\"RefusalReasons\"].apply(NPD_regex)\n",
    "# create a column containing a string of references to CLP\n",
    "df['CLPs Referenced'] = df[\"RefusalReasons\"].apply(CLP_regex)\n",
    "# create a column containing a string of permission references\n",
    "df['Perms Referenced'] = df[\"RefusalReasons\"].apply(Perm_regex)\n",
    "# create a column containing a string of AONB references\n",
    "df['AONB Referenced'] = df[\"RefusalReasons\"].apply(AONB_regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be8ecc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from an initial list which could contain lists itself, return a single list containing\n",
    "# all values in the column, cast to a set to remove duplicates\n",
    "all_types_list = set(list_all_values_in_col('Dev_Type'))\n",
    "all_perms_list = set(list_all_values_in_col('Perms Referenced'))\n",
    "all_NPs_list = set(list_all_values_in_col('NDPs Referenced'))\n",
    "all_CLPs_list = set(list_all_values_in_col('CLPs Referenced'))\n",
    "all_AONB_list = set(list_all_values_in_col('AONB Referenced'))\n",
    "\n",
    "# create a list of all these sets bar Dev_Type (kept) and AONB (cleaned)\n",
    "master_list = [all_perms_list, all_NPs_list, all_CLPs_list]\n",
    "# for each of the lists of sets, add all the values together, then convert back to a list for sorting\n",
    "master_list = list(set().union(*master_list))\n",
    "# add the extracted phrases to remove to the more generic phrases you have manually added on exploration\n",
    "master_list = text_to_remove + text_to_remove2 + master_list\n",
    "# sort the values by largest number of characters first (so .replace will be greedy)\n",
    "master_list.sort(reverse = True, key=leng_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "29511dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tidy the various AONB references to return only 'AONB'\n",
    "# replace null values with empty string\n",
    "df['AONB Referenced'].fillna('', inplace=True)\n",
    "# update non-null values with 'AONB'\n",
    "df.loc[df['AONB Referenced'] != '', 'AONB Referenced'] = 'AONB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "81420f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace all iterations of AONB with 'AONB' in the main data\n",
    "df['RefusalReasons'] = df['RefusalReasons'].apply(replace_text, args=(all_AONB_list, 'AONB'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c4472ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dev_Type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Minor - Dwelling</th>\n",
       "      <td>1048</td>\n",
       "      <td>1</td>\n",
       "      <td>Minor - Dwelling</td>\n",
       "      <td>1048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Householder</th>\n",
       "      <td>257</td>\n",
       "      <td>1</td>\n",
       "      <td>Householder</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All other minor developments</th>\n",
       "      <td>154</td>\n",
       "      <td>1</td>\n",
       "      <td>All other minor developments</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLEUD/CLOPED</th>\n",
       "      <td>144</td>\n",
       "      <td>1</td>\n",
       "      <td>CLEUD/CLOPED</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Changes of Use</th>\n",
       "      <td>103</td>\n",
       "      <td>1</td>\n",
       "      <td>Changes of Use</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TPO applications</th>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "      <td>TPO applications</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Smallscale Major Dwellings</th>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>Smallscale Major Dwellings</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Minor - Dwelling - PIP apps only</th>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>Minor - Dwelling - PIP apps only</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Listed Building Consent (alter/extend)</th>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>Listed Building Consent (alter/extend)</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All other small scale major developments</th>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>All other small scale major developments</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Not required - S106/S52</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>Not required - S106/S52</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Advertisements</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>Advertisements</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Minor - Gypsy and Traveller Sites</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>Minor - Gypsy and Traveller Sites</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All other largescale major developments</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>All other largescale major developments</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Minor - Retail distribution/servicing</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Minor - Retail distribution/servicing</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Not Required - Hedgerow Removal</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Not Required - Hedgerow Removal</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Notification - TEL type</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Notification - TEL type</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prior Approval - AF2 type</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Prior Approval - AF2 type</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Notification - AF types - Bldgs/tracks</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Notification - AF types - Bldgs/tracks</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Largescale Major Dwellings</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Largescale Major Dwellings</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Largescale Major Retail Distri/Servicing</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Largescale Major Retail Distri/Servicing</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Not required - General</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Not required - General</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Smallscale Major Retail Distri/Servicing</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Smallscale Major Retail Distri/Servicing</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCA applications</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>TCA applications</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Minor - Offices/R&amp;D/Light Industry</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Minor - Offices/R&amp;D/Light Industry</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Largescale Major Hvy Ind/Storage/Whouse</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Largescale Major Hvy Ind/Storage/Whouse</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Notification - Rail type</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Notification - Rail type</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Exception Notice Not Reqd (was 5day not)</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Exception Notice Not Reqd (was 5day not)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minor - dwelling</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>minor - dwelling</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         count unique  \\\n",
       "Dev_Type                                                \n",
       "Minor - Dwelling                          1048      1   \n",
       "Householder                                257      1   \n",
       "All other minor developments               154      1   \n",
       "CLEUD/CLOPED                               144      1   \n",
       "Changes of Use                             103      1   \n",
       "TPO applications                            78      1   \n",
       "Smallscale Major Dwellings                  66      1   \n",
       "Minor - Dwelling - PIP apps only            64      1   \n",
       "Listed Building Consent (alter/extend)      48      1   \n",
       "All other small scale major developments    21      1   \n",
       "Not required - S106/S52                     20      1   \n",
       "Advertisements                              11      1   \n",
       "Minor - Gypsy and Traveller Sites            9      1   \n",
       "All other largescale major developments      8      1   \n",
       "Minor - Retail distribution/servicing        5      1   \n",
       "Not Required - Hedgerow Removal              5      1   \n",
       "Notification - TEL type                      5      1   \n",
       "Prior Approval - AF2 type                    5      1   \n",
       "Notification - AF types - Bldgs/tracks       4      1   \n",
       "Largescale Major Dwellings                   4      1   \n",
       "Largescale Major Retail Distri/Servicing     2      1   \n",
       "Not required - General                       2      1   \n",
       "Smallscale Major Retail Distri/Servicing     2      1   \n",
       "TCA applications                             2      1   \n",
       "Minor - Offices/R&D/Light Industry           1      1   \n",
       "Largescale Major Hvy Ind/Storage/Whouse      1      1   \n",
       "Notification - Rail type                     1      1   \n",
       "Exception Notice Not Reqd (was 5day not)     1      1   \n",
       "minor - dwelling                             1      1   \n",
       "\n",
       "                                                                               top  \\\n",
       "Dev_Type                                                                             \n",
       "Minor - Dwelling                                                  Minor - Dwelling   \n",
       "Householder                                                            Householder   \n",
       "All other minor developments                          All other minor developments   \n",
       "CLEUD/CLOPED                                                          CLEUD/CLOPED   \n",
       "Changes of Use                                                      Changes of Use   \n",
       "TPO applications                                                  TPO applications   \n",
       "Smallscale Major Dwellings                              Smallscale Major Dwellings   \n",
       "Minor - Dwelling - PIP apps only                  Minor - Dwelling - PIP apps only   \n",
       "Listed Building Consent (alter/extend)      Listed Building Consent (alter/extend)   \n",
       "All other small scale major developments  All other small scale major developments   \n",
       "Not required - S106/S52                                    Not required - S106/S52   \n",
       "Advertisements                                                      Advertisements   \n",
       "Minor - Gypsy and Traveller Sites                Minor - Gypsy and Traveller Sites   \n",
       "All other largescale major developments    All other largescale major developments   \n",
       "Minor - Retail distribution/servicing        Minor - Retail distribution/servicing   \n",
       "Not Required - Hedgerow Removal                    Not Required - Hedgerow Removal   \n",
       "Notification - TEL type                                    Notification - TEL type   \n",
       "Prior Approval - AF2 type                                Prior Approval - AF2 type   \n",
       "Notification - AF types - Bldgs/tracks      Notification - AF types - Bldgs/tracks   \n",
       "Largescale Major Dwellings                              Largescale Major Dwellings   \n",
       "Largescale Major Retail Distri/Servicing  Largescale Major Retail Distri/Servicing   \n",
       "Not required - General                                      Not required - General   \n",
       "Smallscale Major Retail Distri/Servicing  Smallscale Major Retail Distri/Servicing   \n",
       "TCA applications                                                  TCA applications   \n",
       "Minor - Offices/R&D/Light Industry              Minor - Offices/R&D/Light Industry   \n",
       "Largescale Major Hvy Ind/Storage/Whouse    Largescale Major Hvy Ind/Storage/Whouse   \n",
       "Notification - Rail type                                  Notification - Rail type   \n",
       "Exception Notice Not Reqd (was 5day not)  Exception Notice Not Reqd (was 5day not)   \n",
       "minor - dwelling                                                  minor - dwelling   \n",
       "\n",
       "                                          freq  \n",
       "Dev_Type                                        \n",
       "Minor - Dwelling                          1048  \n",
       "Householder                                257  \n",
       "All other minor developments               154  \n",
       "CLEUD/CLOPED                               144  \n",
       "Changes of Use                             103  \n",
       "TPO applications                            78  \n",
       "Smallscale Major Dwellings                  66  \n",
       "Minor - Dwelling - PIP apps only            64  \n",
       "Listed Building Consent (alter/extend)      48  \n",
       "All other small scale major developments    21  \n",
       "Not required - S106/S52                     20  \n",
       "Advertisements                              11  \n",
       "Minor - Gypsy and Traveller Sites            9  \n",
       "All other largescale major developments      8  \n",
       "Minor - Retail distribution/servicing        5  \n",
       "Not Required - Hedgerow Removal              5  \n",
       "Notification - TEL type                      5  \n",
       "Prior Approval - AF2 type                    5  \n",
       "Notification - AF types - Bldgs/tracks       4  \n",
       "Largescale Major Dwellings                   4  \n",
       "Largescale Major Retail Distri/Servicing     2  \n",
       "Not required - General                       2  \n",
       "Smallscale Major Retail Distri/Servicing     2  \n",
       "TCA applications                             2  \n",
       "Minor - Offices/R&D/Light Industry           1  \n",
       "Largescale Major Hvy Ind/Storage/Whouse      1  \n",
       "Notification - Rail type                     1  \n",
       "Exception Notice Not Reqd (was 5day not)     1  \n",
       "minor - dwelling                             1  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show how many records there are for each development category\n",
    "df.groupby('Dev_Type')['Dev_Type'].describe().sort_values(by='freq', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "556d67c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''NOT SURE IF I WANT TO DO THIS YET - THIS CREATES A DATAFRAME FOR EVERY CATEGORY IN THE LIST\n",
    "    ALLOWS YOU TO EXPLORE THE DATA A LITTLE EASIER TO SEE WHAT TO INCLUDE/EXCLUDE'''\n",
    "df_dict ={}\n",
    "for item in all_types_list:\n",
    "    df_dict[item] = df[df['Dev_Type']==item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "57feda11",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''First cut the source data down by exluding categories that don't share common refusal reasons - e.g.\n",
    "very specific types of development'''\n",
    "\n",
    "# maybe exclude:\n",
    "# CLUED - Certificate of Lawful Development - these generally refer to whether the use of something\n",
    "# is lawful or not, so typically refusal reasons differ here\n",
    "# TPO/TPA - Tree Protection  - generally refusals are for public amenity reasons\n",
    "#'Listed Building Consent (alter/extend)' - borderline one because it's to do with alterations/extensions\n",
    "\n",
    "exclude_cats = ['CLEUD/CLOPED','TPO applications','Not Required - Hedgerow Removal','TCA applications']\n",
    "# Prior Approval - AF2 type == agricultural stuff?\n",
    "borderline_cats = ['Listed Building Consent (alter/extend)','Prior Approval - AF2 type','Notification - Rail type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "07a8eb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter categories to exclude from the df. '~' negates/gives you the opposite of something\n",
    "df = df[~df['Dev_Type'].isin(exclude_cats)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "96363faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as you've removed entries from the index, they will be out of order, reset to fix\n",
    "# also required when applying boolean masks later, or when merging on results by index later\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2166d836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intital text stripping, including phrases as opposed words and common sentences\n",
    "# plus stripping of perm references etc, plus stripping of lemmatized versions of text_to_remove\n",
    "df['cleaned'] = df['RefusalReasons'].apply(strip_specific_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1a3c463b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned'] = df['cleaned'].apply(strip_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "46d00212",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned'] = df['cleaned'].apply(stopwords_and_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "911f8954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'cleaned' is an object datatype, cast as str to allow you to search for substrings for refinement\n",
    "df['checking col'] = df['cleaned'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1dfae9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# allows you to investigate word fragements and oddities to clean them up in future iterations\n",
    "#df_topic_dict[5]['cleaned'] = df_topic_dict[5]['cleaned'].astype(str)\n",
    "#df_topic_dict[5].columns\n",
    "#df_topic_dict[5][df_topic_dict[5]['cleaned'].str.contains('ination')]\n",
    "#df[df['checking col'].str.contains('beauty')]\n",
    "#df[~df['test'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ed5a850f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of lists of the preprocessed docs, where each list is the preprocessed text\n",
    "processed_docs = df['cleaned'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6ba742d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dict from 'processed_docs'; each word and an integer ID. Accessed as usual dictionary[0]\n",
    "# Later passed to the model for training. Besides the id and word, it also contains frequency info\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ee866e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL STEP Remove very rare and very common words: appearing < 15 times, > 30% of all documents\n",
    "# keep_n=100000 == only the 100,000 most frequent tokens in the corpus will be kept\n",
    "#dictionary.filter_extremes(no_below=15, no_above=0.2, keep_n= 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a9718fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create BOW model for each doc i.e for each we create a list of tuples (int_ref, count)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "44c86fac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nExpected topics might be something like:\\n\\nAONB,\\nCharacter, Location, Visual Harm, Appearance\\nFlooding/Drainage\\nAccess/Parking\\nWHS, Historic Environment, Listed Buildings\\nConservation/Protected Areas\\nDesign, Scale, overdevelopment\\nNieghbouring property and amenity\\n\\n'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"MANUALLY CHANGE NUMBER OF TOPICS AND PASSES TO REFINE MODEL HERE. WILL BE PASSED TO MODEL IN NEXT STEP\"\"\"\n",
    "num_topics = 8\n",
    "num_passes = 500\n",
    "\"\"\"\n",
    "Expected topics might be something like:\n",
    "\n",
    "AONB,\n",
    "Character, Location, Visual Harm, Appearance\n",
    "Flooding/Drainage\n",
    "Access/Parking\n",
    "WHS, Historic Environment, Listed Buildings\n",
    "Conservation/Protected Areas\n",
    "Design, Scale, overdevelopment\n",
    "Nieghbouring property and amenity\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "dde3e6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    #Train your lda model using gensim.models.LdaMulticore and save it to 'lda_model'\n",
    "    #id2word is a mapping from word ids (integers) to words (strings)\n",
    "    lda_model =  gensim.models.LdaMulticore(bow_corpus, \n",
    "                                       num_topics = num_topics, # select no of topics to try and create from samples\n",
    "                                       id2word = dictionary, # above comment                                 \n",
    "                                       passes = num_passes, # number of passes the model with make (> passes = more thorough????)\n",
    "                                       workers = 2) # no of extra processes to use for parallelization. Uses all available cores by default\n",
    "    return lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "082cf9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_topics_df():\n",
    "    topic_words = {}\n",
    "    for topic_num in range(num_topics):\n",
    "        words_probs = lda_model.show_topic(topic_num, topn=10)\n",
    "        words = [word for word, _ in words_probs]\n",
    "        probs = [prob for _, prob in words_probs]\n",
    "        topic_words[f\"Topic {topic_num + 1}\"] = words + probs\n",
    "\n",
    "    topics_df = pd.DataFrame(topic_words)\n",
    "    \n",
    "    return topics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea654643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this if you know the num_topics and num_passes you want to use\n",
    "\n",
    "#lda_model = train_model()\n",
    "#topics_df = make_topics_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "394247e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary to house iterations of the model and it's various passes, topic numbers and top 10 words\n",
    "topics_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1101151c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passes: 50, Topics: 5\n",
      "Passes: 50, Topics: 6\n",
      "Passes: 50, Topics: 7\n",
      "Passes: 50, Topics: 8\n",
      "Passes: 50, Topics: 9\n",
      "Passes: 50, Topics: 10\n",
      "Passes: 100, Topics: 5\n",
      "Passes: 100, Topics: 6\n",
      "Passes: 100, Topics: 7\n",
      "Passes: 100, Topics: 8\n",
      "Passes: 100, Topics: 9\n",
      "Passes: 100, Topics: 10\n"
     ]
    }
   ],
   "source": [
    "# for passes ranging from 50 to 450, adding 100 passes each time\n",
    "#for i in range(50, 450, 100):\n",
    "for i in range(50,150,50):\n",
    "    num_passes = i\n",
    "    # for a number of topics ranging from 5 to 10\n",
    "    for b in range(5,11):\n",
    "        num_topics = b\n",
    "        print(\"Passes: {}, Topics: {}\".format(num_passes, num_topics))\n",
    "        scenario = \"P_\" + str(num_passes) + \"_T_\" + str(num_topics)\n",
    "        lda_model = train_model()\n",
    "        topics_dict[scenario] = make_topics_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8cf151fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic 1</th>\n",
       "      <th>Topic 2</th>\n",
       "      <th>Topic 3</th>\n",
       "      <th>Topic 4</th>\n",
       "      <th>Topic 5</th>\n",
       "      <th>Topic 6</th>\n",
       "      <th>Topic 7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>form</td>\n",
       "      <td>conservation</td>\n",
       "      <td>need</td>\n",
       "      <td>property</td>\n",
       "      <td>countryside</td>\n",
       "      <td>aonb</td>\n",
       "      <td>access</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>character</td>\n",
       "      <td>list</td>\n",
       "      <td>affordable</td>\n",
       "      <td>neighbour</td>\n",
       "      <td>character</td>\n",
       "      <td>heritage</td>\n",
       "      <td>absence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>design</td>\n",
       "      <td>harm</td>\n",
       "      <td>community</td>\n",
       "      <td>overlook</td>\n",
       "      <td>form</td>\n",
       "      <td>landscape</td>\n",
       "      <td>suitable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fail</td>\n",
       "      <td>character</td>\n",
       "      <td>market</td>\n",
       "      <td>design</td>\n",
       "      <td>location</td>\n",
       "      <td>harm</td>\n",
       "      <td>safe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neighbour</td>\n",
       "      <td>fail</td>\n",
       "      <td>business</td>\n",
       "      <td>noise</td>\n",
       "      <td>rural</td>\n",
       "      <td>character</td>\n",
       "      <td>flood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>amenity</td>\n",
       "      <td>appearance</td>\n",
       "      <td>fail</td>\n",
       "      <td>occupiers</td>\n",
       "      <td>open</td>\n",
       "      <td>world</td>\n",
       "      <td>fail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>scale</td>\n",
       "      <td>public</td>\n",
       "      <td>open</td>\n",
       "      <td>increase</td>\n",
       "      <td>residential</td>\n",
       "      <td>management</td>\n",
       "      <td>risk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>exist</td>\n",
       "      <td>preserve</td>\n",
       "      <td>agricultural</td>\n",
       "      <td>balcony</td>\n",
       "      <td>harm</td>\n",
       "      <td>fail</td>\n",
       "      <td>highway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>appearance</td>\n",
       "      <td>historic</td>\n",
       "      <td>evidence</td>\n",
       "      <td>guide</td>\n",
       "      <td>special</td>\n",
       "      <td>conserve</td>\n",
       "      <td>provision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>mass</td>\n",
       "      <td>set</td>\n",
       "      <td>purpose</td>\n",
       "      <td>disturbance</td>\n",
       "      <td>landscape</td>\n",
       "      <td>set</td>\n",
       "      <td>visibility</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.026311</td>\n",
       "      <td>0.050181</td>\n",
       "      <td>0.028049</td>\n",
       "      <td>0.020935</td>\n",
       "      <td>0.041962</td>\n",
       "      <td>0.036284</td>\n",
       "      <td>0.031582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.026202</td>\n",
       "      <td>0.036878</td>\n",
       "      <td>0.016767</td>\n",
       "      <td>0.020744</td>\n",
       "      <td>0.029014</td>\n",
       "      <td>0.034702</td>\n",
       "      <td>0.01764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.025526</td>\n",
       "      <td>0.032726</td>\n",
       "      <td>0.014781</td>\n",
       "      <td>0.02049</td>\n",
       "      <td>0.02382</td>\n",
       "      <td>0.034416</td>\n",
       "      <td>0.015829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.023868</td>\n",
       "      <td>0.029716</td>\n",
       "      <td>0.01425</td>\n",
       "      <td>0.019788</td>\n",
       "      <td>0.022883</td>\n",
       "      <td>0.025208</td>\n",
       "      <td>0.015577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.019247</td>\n",
       "      <td>0.022444</td>\n",
       "      <td>0.014223</td>\n",
       "      <td>0.017819</td>\n",
       "      <td>0.022603</td>\n",
       "      <td>0.020507</td>\n",
       "      <td>0.013536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.017986</td>\n",
       "      <td>0.022173</td>\n",
       "      <td>0.014011</td>\n",
       "      <td>0.014999</td>\n",
       "      <td>0.020718</td>\n",
       "      <td>0.020404</td>\n",
       "      <td>0.013352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.016687</td>\n",
       "      <td>0.020769</td>\n",
       "      <td>0.013606</td>\n",
       "      <td>0.01484</td>\n",
       "      <td>0.018793</td>\n",
       "      <td>0.018665</td>\n",
       "      <td>0.012595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.014883</td>\n",
       "      <td>0.018984</td>\n",
       "      <td>0.013602</td>\n",
       "      <td>0.014313</td>\n",
       "      <td>0.017327</td>\n",
       "      <td>0.013999</td>\n",
       "      <td>0.012391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0132</td>\n",
       "      <td>0.017849</td>\n",
       "      <td>0.013021</td>\n",
       "      <td>0.014153</td>\n",
       "      <td>0.016474</td>\n",
       "      <td>0.012738</td>\n",
       "      <td>0.010548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.012522</td>\n",
       "      <td>0.016305</td>\n",
       "      <td>0.010857</td>\n",
       "      <td>0.013401</td>\n",
       "      <td>0.014245</td>\n",
       "      <td>0.012242</td>\n",
       "      <td>0.010106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Topic 1       Topic 2       Topic 3      Topic 4      Topic 5  \\\n",
       "0         form  conservation          need     property  countryside   \n",
       "1    character          list    affordable    neighbour    character   \n",
       "2       design          harm     community     overlook         form   \n",
       "3         fail     character        market       design     location   \n",
       "4    neighbour          fail      business        noise        rural   \n",
       "5      amenity    appearance          fail    occupiers         open   \n",
       "6        scale        public          open     increase  residential   \n",
       "7        exist      preserve  agricultural      balcony         harm   \n",
       "8   appearance      historic      evidence        guide      special   \n",
       "9         mass           set       purpose  disturbance    landscape   \n",
       "10    0.026311      0.050181      0.028049     0.020935     0.041962   \n",
       "11    0.026202      0.036878      0.016767     0.020744     0.029014   \n",
       "12    0.025526      0.032726      0.014781      0.02049      0.02382   \n",
       "13    0.023868      0.029716       0.01425     0.019788     0.022883   \n",
       "14    0.019247      0.022444      0.014223     0.017819     0.022603   \n",
       "15    0.017986      0.022173      0.014011     0.014999     0.020718   \n",
       "16    0.016687      0.020769      0.013606      0.01484     0.018793   \n",
       "17    0.014883      0.018984      0.013602     0.014313     0.017327   \n",
       "18      0.0132      0.017849      0.013021     0.014153     0.016474   \n",
       "19    0.012522      0.016305      0.010857     0.013401     0.014245   \n",
       "\n",
       "       Topic 6     Topic 7  \n",
       "0         aonb      access  \n",
       "1     heritage     absence  \n",
       "2    landscape    suitable  \n",
       "3         harm        safe  \n",
       "4    character       flood  \n",
       "5        world        fail  \n",
       "6   management        risk  \n",
       "7         fail     highway  \n",
       "8     conserve   provision  \n",
       "9          set  visibility  \n",
       "10    0.036284    0.031582  \n",
       "11    0.034702     0.01764  \n",
       "12    0.034416    0.015829  \n",
       "13    0.025208    0.015577  \n",
       "14    0.020507    0.013536  \n",
       "15    0.020404    0.013352  \n",
       "16    0.018665    0.012595  \n",
       "17    0.013999    0.012391  \n",
       "18    0.012738    0.010548  \n",
       "19    0.012242    0.010106  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#topics_dict['P_100T_7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "37ed11c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get todays date in format (dd_mm_yy)\n",
    "today = \"(\" + date.today().strftime(\"%d_%m_%Y\") + \")\"\n",
    "# create filename with variables used in run and the date\n",
    "filename = \"LDA_topics_\" + str(num_topics) + \"_passes_\" + str(num_passes) + today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "bec6a8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an Excel writer object\n",
    "writer = pd.ExcelWriter(filename + '.xlsx', engine='xlsxwriter')\n",
    "\n",
    "# loop over the dictionary and write each dataframe to a separate worksheet\n",
    "for sheet_name, df in topics_dict.items():\n",
    "    df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "    \n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "14789d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.071*\"access\" + 0.028*\"safe\" + 0.027*\"highway\" + 0.023*\"park\" + 0.022*\"visibility\" + 0.022*\"suitable\" + 0.019*\"vehicles\" + 0.018*\"road\" + 0.016*\"fail\" + 0.015*\"increase\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.059*\"countryside\" + 0.039*\"form\" + 0.030*\"special\" + 0.028*\"character\" + 0.026*\"absence\" + 0.025*\"location\" + 0.025*\"open\" + 0.019*\"residential\" + 0.019*\"rural\" + 0.017*\"unsustainable\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.020*\"need\" + 0.017*\"affordable\" + 0.013*\"delivery\" + 0.012*\"fail\" + 0.011*\"strategic\" + 0.010*\"scale\" + 0.010*\"open\" + 0.009*\"absence\" + 0.009*\"suitable\" + 0.008*\"adverse\"\n",
      "\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.030*\"character\" + 0.028*\"design\" + 0.022*\"fail\" + 0.019*\"conservation\" + 0.019*\"appearance\" + 0.017*\"form\" + 0.016*\"harm\" + 0.015*\"neighbour\" + 0.013*\"scale\" + 0.012*\"list\"\n",
      "\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.020*\"provision\" + 0.016*\"absence\" + 0.016*\"fail\" + 0.015*\"secure\" + 0.014*\"species\" + 0.013*\"space\" + 0.012*\"protect\" + 0.011*\"conservation\" + 0.011*\"future\" + 0.011*\"mechanism\"\n",
      "\n",
      "\n",
      "Topic: 5 \n",
      "Words: 0.058*\"heritage\" + 0.035*\"harm\" + 0.031*\"flood\" + 0.031*\"world\" + 0.022*\"risk\" + 0.018*\"historic\" + 0.017*\"mine\" + 0.017*\"substantial\" + 0.017*\"public\" + 0.017*\"outweigh\"\n",
      "\n",
      "\n",
      "Topic: 6 \n",
      "Words: 0.019*\"noise\" + 0.014*\"affordable\" + 0.012*\"market\" + 0.012*\"exist\" + 0.011*\"residential\" + 0.011*\"submit\" + 0.010*\"obligation\" + 0.010*\"purpose\" + 0.009*\"need\" + 0.009*\"longer\"\n",
      "\n",
      "\n",
      "Topic: 7 \n",
      "Words: 0.032*\"character\" + 0.029*\"landscape\" + 0.024*\"countryside\" + 0.023*\"aonb\" + 0.021*\"rural\" + 0.018*\"location\" + 0.018*\"harm\" + 0.014*\"open\" + 0.014*\"set\" + 0.014*\"beauty\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TWEAKING - For each topic, explore the words occuring in that topic and thier relative weights\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    df[topic] = topic\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3902c811",
   "metadata": {},
   "outputs": [],
   "source": [
    "#topics_df.sort_values(by=[5],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e1592ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.071*\"access\" + 0.028*\"safe\" + 0.027*\"highway\" + 0.023*\"park\" + 0.022*\"visibility\" + 0.022*\"suitable\" + 0.019*\"vehicles\" + 0.018*\"road\" + 0.016*\"fail\" + 0.015*\"increase\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.059*\"countryside\" + 0.039*\"form\" + 0.030*\"special\" + 0.028*\"character\" + 0.026*\"absence\" + 0.025*\"location\" + 0.025*\"open\" + 0.019*\"residential\" + 0.019*\"rural\" + 0.017*\"unsustainable\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.020*\"need\" + 0.017*\"affordable\" + 0.013*\"delivery\" + 0.012*\"fail\" + 0.011*\"strategic\" + 0.010*\"scale\" + 0.010*\"open\" + 0.009*\"absence\" + 0.009*\"suitable\" + 0.008*\"adverse\"\n",
      "\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.030*\"character\" + 0.028*\"design\" + 0.022*\"fail\" + 0.019*\"conservation\" + 0.019*\"appearance\" + 0.017*\"form\" + 0.016*\"harm\" + 0.015*\"neighbour\" + 0.013*\"scale\" + 0.012*\"list\"\n",
      "\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.020*\"provision\" + 0.016*\"absence\" + 0.016*\"fail\" + 0.015*\"secure\" + 0.014*\"species\" + 0.013*\"space\" + 0.012*\"protect\" + 0.011*\"conservation\" + 0.011*\"future\" + 0.011*\"mechanism\"\n",
      "\n",
      "\n",
      "Topic: 5 \n",
      "Words: 0.058*\"heritage\" + 0.035*\"harm\" + 0.031*\"flood\" + 0.031*\"world\" + 0.022*\"risk\" + 0.018*\"historic\" + 0.017*\"mine\" + 0.017*\"substantial\" + 0.017*\"public\" + 0.017*\"outweigh\"\n",
      "\n",
      "\n",
      "Topic: 6 \n",
      "Words: 0.019*\"noise\" + 0.014*\"affordable\" + 0.012*\"market\" + 0.012*\"exist\" + 0.011*\"residential\" + 0.011*\"submit\" + 0.010*\"obligation\" + 0.010*\"purpose\" + 0.009*\"need\" + 0.009*\"longer\"\n",
      "\n",
      "\n",
      "Topic: 7 \n",
      "Words: 0.032*\"character\" + 0.029*\"landscape\" + 0.024*\"countryside\" + 0.023*\"aonb\" + 0.021*\"rural\" + 0.018*\"location\" + 0.018*\"harm\" + 0.014*\"open\" + 0.014*\"set\" + 0.014*\"beauty\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TWEAKING - For each topic, explore the words occuring in that topic and thier relative weights\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "01ab4ae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" THIS VERSION ALLOWS YOU TO WEIGHT SEED WORDS FOR EACH TOPIC TO HELP TO PROCESS\\n# Define the seed word lists for each topic\\nseed_words_topic1 = ['flood', 'drainage', 'risk']\\nseed_words_topic2 = ['access', 'safe', 'highway','visability', 'vehicle']\\nseed_words_topic3 = ['countryside', 'open', 'rural', 'location','aonb']\\nseed_words_topic4 = ['heritage', 'world', 'historic']\\nseed_words_topic5 = ['conservation', 'special', 'enhance','protect']\\nseed_words_topic6 = ['neighbour', 'amenities', 'overlook', 'overbear']\\nseed_words_topic7 = ['design', 'scale', 'mass']\\n# Define more seed word lists for other topics if needed\\n\\nnum_topics = 7\\n\\n# Train the LDA model using the seed words for each topic\\nlda_model = gensim.models.LdaModel(bow_corpus, num_topics=num_topics,\\n                                   id2word=dictionary,\\n                                   passes=10,\\n                                   alpha=[0.01]*num_topics,\\n                                   eta=[[0.8 if word in seed_words_topic1 else 0.001 for word in dictionary.token2id],\\n                                        [0.8 if word in seed_words_topic2 else 0.001 for word in dictionary.token2id],\\n                                        [0.8 if word in seed_words_topic3 else 0.001 for word in dictionary.token2id],\\n                                        [0.8 if word in seed_words_topic4 else 0.001 for word in dictionary.token2id],\\n                                        [0.8 if word in seed_words_topic5 else 0.001 for word in dictionary.token2id],\\n                                        [0.8 if word in seed_words_topic6 else 0.001 for word in dictionary.token2id],\\n                                        [0.8 if word in seed_words_topic7 else 0.001 for word in dictionary.token2id]])\\n\\n# Print the topics with their top 10 most important words\\nfor idx, topic in lda_model.print_topics(num_topics=num_topics, num_words=10):\\n    print('Topic: {} \\nWords: {}'.format(idx, topic))\\n    print('\\n')\\n\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def seed_lda_multi():\n",
    "    \n",
    "    # THIS VERSION ALLOWS YOU TO WEIGHT SEED WORDS FOR EACH TOPIC TO HELP TO PROCESS\n",
    "    # Define the seed word lists for each topic\n",
    "    seed_words_topic1 = ['flood', 'drainage', 'risk']\n",
    "    seed_words_topic2 = ['access', 'safe', 'highway','visability', 'vehicle']\n",
    "    seed_words_topic3 = ['countryside', 'open', 'rural', 'location','aonb']\n",
    "    seed_words_topic4 = ['heritage', 'world', 'historic']\n",
    "    seed_words_topic5 = ['conservation', 'special', 'enhance','protect']\n",
    "    seed_words_topic6 = ['neighbour', 'amenities', 'overlook', 'overbear']\n",
    "    seed_words_topic7 = ['design', 'scale', 'mass']\n",
    "    # Define more seed word lists for other topics if needed\n",
    "\n",
    "    num_topics = 7\n",
    "\n",
    "    # Train the LDA model using the seed words for each topic\n",
    "    lda_model = gensim.models.LdaModel(bow_corpus, num_topics=num_topics,\n",
    "                                       id2word=dictionary,\n",
    "                                       passes=10,\n",
    "                                       alpha=[0.01]*num_topics,\n",
    "                                       eta=[[0.8 if word in seed_words_topic1 else 0.001 for word in dictionary.token2id],\n",
    "                                            [0.8 if word in seed_words_topic2 else 0.001 for word in dictionary.token2id],\n",
    "                                            [0.8 if word in seed_words_topic3 else 0.001 for word in dictionary.token2id],\n",
    "                                            [0.8 if word in seed_words_topic4 else 0.001 for word in dictionary.token2id],\n",
    "                                            [0.8 if word in seed_words_topic5 else 0.001 for word in dictionary.token2id],\n",
    "                                            [0.8 if word in seed_words_topic6 else 0.001 for word in dictionary.token2id],\n",
    "                                            [0.8 if word in seed_words_topic7 else 0.001 for word in dictionary.token2id]])\n",
    "\n",
    "    # Print the topics with their top 10 most important words\n",
    "    for idx, topic in lda_model.print_topics(num_topics=num_topics, num_words=10):\n",
    "        print('Topic: {} \\nWords: {}'.format(idx, topic))\n",
    "        print('\\n')\n",
    "        \n",
    "    return lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8e8b3ae2",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2968104820.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/g4/65v8hcw105j4mynw26wj6f3r0000gp/T/ipykernel_16460/2968104820.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    def seed_lda_single()\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def seed_lda_single()\n",
    "\n",
    "    # THIS VERSION ALLOWS YOU TO PROVIDE A LIST OF SEED WORDS TO WEIGHT MORE HIGHLY\n",
    "    # Specify seed topics that are particularly important in assigning topics\n",
    "    seed_topic = ['aonb','flood','heritage','conservation','neighbour','countryside', 'affordable']\n",
    "\n",
    "    # Set the weight of the above seed topics to be high as default - 0.5\n",
    "    for word in seed_topic:\n",
    "        for i in range(lda_model.num_topics):\n",
    "            lda_model.get_topic_terms(i)\n",
    "            if word in dict(lda_model.get_topic_terms(i)):\n",
    "                lda_model.get_topic_terms(i)[dict(lda_model.get_topic_terms(i)).get(word)][1] = 1\n",
    "\n",
    "    # re-train the LDA model with the updated topic distributions\n",
    "    lda_model.update(bow_corpus)\n",
    "\n",
    "    # TWEAKING - For each topic, explore the words occuring in that topic and thier relative weights\n",
    "    for idx, topic in lda_model.print_topics(-1):\n",
    "        print(\"Topic: {} \\nWords: {}\".format(idx, topic))\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    return lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0319b3a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1980341942.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/g4/65v8hcw105j4mynw26wj6f3r0000gp/T/ipykernel_16460/1980341942.py\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    \"\"\"THIS IS A MANUAL EDIT YOU NEED TO MAKE\"\"\"\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\"\"\"# making sense of the derived topics the model suggests\n",
    "\n",
    "\n",
    "\"\"\"THIS IS A MANUAL EDIT YOU NEED TO MAKE\"\"\"\n",
    "\n",
    "\n",
    "inferred_topics = {\n",
    "    0: 'Access',\n",
    "    1: 'Conservation/Heritage/Listed',\n",
    "    2: 'Unknown',\n",
    "    3: 'Nieghbours',\n",
    "    4: 'Countryside',\n",
    "    5: 'Character,landscape, AONB',\n",
    "    6: 'Flood/Drainage'\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d611a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Quit code here - manual data entry required for topic naming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f17d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "fsds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7347209f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the topic probabilities for each document in the corpus\n",
    "document_topics = [lda_model.get_document_topics(bow) for bow in bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6887ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to contain dictionaries with topic_num, topic probabilities for each document\n",
    "topic_probs_list = []\n",
    "\n",
    "# Iterate over the documents in the corpus\n",
    "for doc in bow_corpus:\n",
    "    # Get the topic and topic probabilities for the current document\n",
    "    topic_probs = dict(lda_model.get_document_topics(doc))\n",
    "    # Add the topic probabilities to the list\n",
    "    topic_probs_list.append(topic_probs)\n",
    "\n",
    "# Create a Pandas DataFrame from the list of dictionaries\n",
    "topic_probs_df = pd.DataFrame(topic_probs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62328fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the topic probabilities dataframe to the original df by index\n",
    "df = pd.merge(df, topic_probs_df, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a93a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary of dfs where each df has that topic flag to quickly evaluate the groups\n",
    "df_topic_dict ={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9fa216",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,num_topics):\n",
    "    # drop other columns to make it easier to evaulate\n",
    "    rem_cols = ['Ref','Description','DecDate','NDPs Referenced','CLPs Referenced','Perms Referenced','AONB Referenced','checking col']\n",
    "    # sort each topic df by it's topic number (highest probabilities first), after first removing nulls\n",
    "    df_topic_dict[i] = df[~df[i].isnull()].sort_values(by=[i], ascending=False)\n",
    "    print(len(df_topic_dict[i]))\n",
    "    \n",
    "    # add other topic columns to the list to be removed to make it easier to see\n",
    "    for col in df_topic_dict[i].columns:\n",
    "        if df_topic_dict[i][col].dtype in ['int64', 'float64'] and col != i and col !='index':\n",
    "            rem_cols.append(col)\n",
    "            df_topic_dict[i]= drop_col_by_labels(df_topic_dict[i],rem_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d422b279",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# change the topic numbers in the header to your meaningful topic titles\n",
    "df = df.rename(columns=inferred_topics)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811d8b8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288e5ebc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d7e5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_csv():\n",
    "    from pathlib import Path\n",
    "    path = \"/Users/GlassShark1/Python/Refusals Data/\"\n",
    "    filepath = Path(path + filename + \".csv\") \n",
    "    print(filepath)\n",
    "    #filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "    df.to_csv(filepath) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf0897e",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db77aee9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e69b76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78fe46e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dcd2b976",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" CORE MODEL VERSION\n",
    "#Train your lda model using gensim.models.LdaMulticore and save it to 'lda_model'\n",
    "#id2word is a mapping from word ids (integers) to words (strings)\n",
    "lda_model =  gensim.models.LdaMulticore(bow_corpus, \n",
    "                                   num_topics = num_topics, # select no of topics to try and create from samples\n",
    "                                   id2word = dictionary, # above comment                                 \n",
    "                                   passes = num_passes, # number of passes the model with make (> passes = more thorough????)\n",
    "                                   workers = 2) # no of extra processes to use for parallelization. Uses all available cores by default\n",
    "                                   \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b032c188",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unseen_document = \"The proposed development raises direct conflict with the requirements of adopted policy in the Cornwall Local Plan as the site is clearly located in the open countryside where new homes will only be permitted where there are special circumstances, none of which have been identified in this particular case. The proposed development would clearly erode the rural character of this location by introducing further built development that would result in material harm to the character and appearance of the countryside, thus not conserving the landscape character and natural beauty of the Area of Outstanding Natural Beauty (AONB) in this location. The site is not a sustainable or accessible location for a new dwelling and the development would therefore be contrary to development plan policy and cause material environmental harm to the rural character of the area, contrary to policies 1, 2, 3, 7, 12, 23 and 27 in the Cornwall Local Plan Strategic Policies 2010 - 2030 \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4832f623",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Data preprocessing step for the unseen document\n",
    "# bow_vector = list of tuples with number representing a word in the corpus and the count of those words???\n",
    "bow_vector = dictionary.doc2bow(stopwords_and_preprocess(unseen_document))\n",
    "\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    # number at the end seems to return n words and the strength of their relationship to topic????\n",
    "    print(\"Score: {}\\n Topic: {}\\n\".format(score, lda_model.print_topic(index, 10)))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250dc55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging files to keep track of model / topics etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6f3837",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Preview BOW for our sample preprocessed document - this is on the 20th record as an example'''\n",
    "\"\"\"\n",
    "document_num = 0 #think this is just the 20th record?\n",
    "\n",
    "bow_doc_x = bow_corpus[document_num]\n",
    "\n",
    "for i in range(len(bow_doc_x)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_x[i][0], \n",
    "                                                     dictionary[bow_doc_x[i][0]], \n",
    "                                                     bow_doc_x[i][1]))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56caa12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"ASK MN: DON'T UNDERSTAND THIS WELL - CAN'T SEE WHERE IT'S APPLIED??\n",
    "\n",
    "alpha and eta are hyperparameters that affect sparsity of the document-topic (theta) and topic-word (lambda) distributions. We will let these be the default values for now(default value is 1/num_topics)\n",
    "\n",
    "Alpha is the per document topic distribution.\n",
    "\n",
    "High alpha: Every document has a mixture of all topics(documents appear similar to each other).\n",
    "Low alpha: Every document has a mixture of very few topics\n",
    "Eta is the per topic word distribution.\n",
    "\n",
    "High eta: Each topic has a mixture of most words(topics appear similar to each other).\n",
    "Low eta: Each topic has a mixture of few words.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0253524",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Checking dictionary created'''\n",
    "\"\"\"count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 100:\n",
    "        break\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488950b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
